{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import plot_param_importances, plot_contour\n",
    "import pandas as pd\n",
    "from pandas import Index\n",
    "from plotly.graph_objects import Figure\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OneHotEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "from modules import dataset, graph, models, paths, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ae1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for asyncio to be run in notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570449bf",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af026745",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47727c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_set: pd.DataFrame = dataset.prepare_dataset('train')\n",
    "valid_set: pd.DataFrame = dataset.prepare_dataset('valid')\n",
    "\n",
    "# Split x and y\n",
    "train_x: pd.DataFrame = train_set.drop(columns=['label'])\n",
    "train_y: pd.Series = train_set['label']\n",
    "valid_x: pd.DataFrame = valid_set.drop(columns=['label'])\n",
    "valid_y: pd.Series = valid_set['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecf074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the similarity graph\n",
    "graph_creation_df: pd.DataFrame = train_x.filter(regex = '_length$')\n",
    "graph_creation_df = graph_creation_df.rename(columns = lambda x: x.replace('_length', ''))\n",
    "similarity_graph: graph.SimilarityGraph = graph.SimilarityGraph(graph_creation_df, threshold = 0.6, connected = True, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# One hot encode categorical columns\n",
    "one_hot_encoder: OneHotEncoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
    "categorical_columns: Index = train_x.select_dtypes(exclude = 'number').columns\n",
    "one_hot_encoder.fit(train_x[categorical_columns])\n",
    "encoded_columns: NDArray[str] = one_hot_encoder.get_feature_names_out(categorical_columns)  # type: ignore\n",
    "train_x_encoded: pd.DataFrame = pd.DataFrame(one_hot_encoder.transform(train_x[categorical_columns]),   # type: ignore\n",
    "                                             columns = encoded_columns\n",
    "                                             )\n",
    "valid_x_encoded: pd.DataFrame = pd.DataFrame(one_hot_encoder.transform(valid_x[categorical_columns]),   # type: ignore\n",
    "                                             columns = encoded_columns\n",
    "                                             )\n",
    "\n",
    "# Scale numerical columns\n",
    "scaler: MinMaxScaler = MinMaxScaler()\n",
    "numerical_columns: Index = train_x.select_dtypes(include = 'number').columns\n",
    "train_x_encoded[numerical_columns] = scaler.fit_transform(train_x[numerical_columns])\n",
    "valid_x_encoded[numerical_columns] = scaler.transform(valid_x[numerical_columns])\n",
    "\n",
    "# Encode labels\n",
    "label_encoder: LabelEncoder = LabelEncoder()\n",
    "train_y_encoded: torch.Tensor = torch.tensor(label_encoder.fit_transform(train_y))\n",
    "valid_y_encoded: torch.Tensor = torch.tensor(label_encoder.transform(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in global and local features\n",
    "global_columns: list[str] = encoded_columns.tolist() + ['sitelinks_count']\n",
    "\n",
    "global_train_x: pd.DataFrame = train_x_encoded[global_columns]\n",
    "global_valid_x: pd.DataFrame = valid_x_encoded[global_columns]\n",
    "\n",
    "local_train_x: pd.DataFrame = train_x_encoded.drop(columns = global_columns)\n",
    "local_valid_x: pd.DataFrame = valid_x_encoded.drop(columns = global_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the data a suitable format for PyTorch Geometric\n",
    "\n",
    "training_graphs: list[nx.Graph] = similarity_graph.get_graphs(local_train_x)\n",
    "validation_graphs: list[nx.Graph] = similarity_graph.get_graphs(local_valid_x)\n",
    "\n",
    "train_data: list[Data] = []\n",
    "global_train_x_tensor: torch.Tensor = torch.from_numpy(global_train_x.to_numpy(dtype = np.float32))\n",
    "for graph_item, global_features, label in zip(training_graphs, global_train_x_tensor, train_y_encoded):\n",
    "    data: Data = from_networkx(graph_item)\n",
    "    data.x_fc = global_features\n",
    "    data.y = label\n",
    "    train_data.append(data)\n",
    "\n",
    "valid_data: list[Data] = []\n",
    "global_valid_x_tensor: torch.Tensor = torch.from_numpy(global_valid_x.to_numpy(dtype = np.float32))\n",
    "for graph_item, global_features, label in zip(validation_graphs, global_valid_x_tensor, valid_y_encoded):\n",
    "    data: Data = from_networkx(graph_item)\n",
    "    data.x_fc = global_features\n",
    "    data.y = label\n",
    "    valid_data.append(data)\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader: DataLoader = DataLoader(train_data, batch_size = 256, shuffle = True)\n",
    "valid_loader: DataLoader = DataLoader(valid_data, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c94dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features for the model\n",
    "\n",
    "n_global_features: int = train_data[0].x_fc.shape[0]\n",
    "n_local_features: int = train_data[0].x_graph.shape[1]\n",
    "n_classes: int = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652f4f1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0ef91",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model obtained during hyperparameter tuning\n",
    "best_overall_checkpoint: ModelCheckpoint = ModelCheckpoint(monitor = 'val_f1', mode = 'max', dirpath = paths.GRAPH_MODEL_DIR, filename = 'graph_{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize the hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    inner_dim: int = trial.suggest_int('inner_dim', 8, 512, log = True)\n",
    "    depth: int = trial.suggest_int('depth', 1, 5)\n",
    "    lr: float = trial.suggest_float('lr', 1e-5, 1e-1, log = True)\n",
    "\n",
    "    # Model\n",
    "    model: models.GraphNet = models.GraphNet(fc_features = n_global_features,\n",
    "                                             node_features = n_local_features,\n",
    "                                             n_classes = n_classes,\n",
    "                                             inner_dim = inner_dim,\n",
    "                                             depth = depth,\n",
    "                                             lr = lr\n",
    "                                             )\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping: EarlyStopping = EarlyStopping(monitor = 'val_f1', mode = 'max', patience = 5)\n",
    "    pruning_callback: PyTorchLightningPruningCallback = PyTorchLightningPruningCallback(trial, monitor = 'val_f1')\n",
    "\n",
    "    # Wandb logger\n",
    "    wandb_logger: WandbLogger = utils.configure_wandb_logger(project = 'Cultural classification on graphs',\n",
    "                                                            name = f'trial_{trial.number}',\n",
    "                                                            config = {'inner_dim': inner_dim,\n",
    "                                                                      'depth': depth,\n",
    "                                                                      'lr': lr\n",
    "                                                                      }\n",
    "                                                            )\n",
    "\n",
    "    # Trainer\n",
    "    trainer: pl.Trainer = pl.Trainer(max_epochs = -1,\n",
    "                                     callbacks = [early_stopping, pruning_callback, best_overall_checkpoint],\n",
    "                                     precision = '16-mixed',\n",
    "                                     logger = wandb_logger,\n",
    "                                     log_every_n_steps = len(train_loader),\n",
    "                                     enable_progress_bar = False,\n",
    "                                     enable_model_summary = False\n",
    "                                     )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, train_loader, valid_loader)\n",
    "    pruning_callback.check_pruned()\n",
    "\n",
    "    # Evaluate the model on the best epoch\n",
    "    f1: float = wandb_logger.experiment.summary['val_f1']['max']\n",
    "\n",
    "    # Close the wandb run\n",
    "    wandb_logger.experiment.finish()\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna study\n",
    "study: optuna.Study = optuna.create_study(direction = 'maximize', pruner = optuna.pruners.MedianPruner(n_warmup_steps = 5))\n",
    "study.optimize(objective, n_trials = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the best model checkpoint\n",
    "best_model_path: Path = utils.rename_best_checkpoint(best_overall_checkpoint, study.best_trial.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb58746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter importances\n",
    "param_importances_fig: Figure = plot_param_importances(study)\n",
    "param_fig: Figure = plot_param_importances(study)\n",
    "param_fig.update_layout(autosize = False,\n",
    "                        width = 1200,\n",
    "                        height = 400\n",
    "                        )\n",
    "param_fig.show()\n",
    "\n",
    "# Plot contour\n",
    "contour_fig: Figure = plot_contour(study)\n",
    "contour_fig.update_layout(autosize = False,\n",
    "                          width = 1200,\n",
    "                          height = 1200\n",
    "                          )\n",
    "contour_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e4940",
   "metadata": {},
   "source": [
    "##### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the best hyperparameters and print them\n",
    "best_params: dict[str, int|float] = study.best_trial.params\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "best_inner_dim: int = int(best_params['inner_dim'])\n",
    "best_depth: int = int(best_params['depth'])\n",
    "best_lr: float = best_params['lr']\n",
    "\n",
    "print(f\"\"\"Best hyperparameters:\n",
    "      \\tlayer dimension: {best_inner_dim}\n",
    "      \\tdepth: {best_depth}\n",
    "      \\tlearning rate: {best_lr:.3e}\"\"\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "models: models.GraphNet = models.GraphNet.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "trainer: pl.Trainer = pl.Trainer(max_epochs = -1, precision = '16-mixed', logger = False)\n",
    "trainer.validate(models, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa895d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "logits: torch.Tensor = torch.cat(trainer.predict(models, valid_loader))    # type: ignore\n",
    "predictions_encoded: torch.Tensor = torch.argmax(logits, dim = 1)\n",
    "utils.plot_confusion_matrix(valid_y_encoded, predictions_encoded, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd25f5",
   "metadata": {},
   "source": [
    "### Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
