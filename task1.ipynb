{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets optuna -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54779b1a",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2400b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import optuna\n",
    "from optuna.visualization import plot_param_importances, plot_contour\n",
    "import pandas as pd\n",
    "from plotly.graph_objects import Figure\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.training_args import TrainingArguments\n",
    "import wandb\n",
    "\n",
    "from modules import dataset, models_creation, paths\n",
    "from modules.utils import model as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for asyncio to be run in notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name: str = 'google/electra-small-discriminator'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f2432",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0a45b",
   "metadata": {},
   "source": [
    "### Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "full_train_set: pd.DataFrame = dataset.extract_dataset('train')\n",
    "full_val_set: pd.DataFrame = dataset.extract_dataset('validation')\n",
    "full_test_set: pd.DataFrame = dataset.extract_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b48d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y\n",
    "train_x: pd.DataFrame = full_train_set[['item', 'description', 'enwiki_extract']]\n",
    "train_y: pd.Series = full_train_set['label']\n",
    "val_x: pd.DataFrame = full_val_set[['item', 'description', 'enwiki_extract']]\n",
    "val_y: pd.Series = full_val_set['label']\n",
    "test_x: pd.DataFrame = full_test_set[['item', 'description', 'enwiki_extract']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93b8af",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc086b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder: LabelEncoder = LabelEncoder()\n",
    "train_y_encoded: pd.Series = pd.Series(label_encoder.fit_transform(train_y),    # type: ignore\n",
    "                                       name = train_y.name,\n",
    "                                       index = train_y.index\n",
    "                                       )\n",
    "val_y_encoded: pd.Series = pd.Series(label_encoder.transform(val_y),    # type: ignore\n",
    "                                     name = val_y.name,\n",
    "                                     index = val_y.index\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "train_df: pd.DataFrame = pd.concat([train_x, train_y_encoded], axis = 1)\n",
    "val_df: pd.DataFrame = pd.concat([val_x, val_y_encoded], axis = 1)\n",
    "test_df: pd.DataFrame = test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8796686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer\n",
    "tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add the special tokens\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<item>', '<description>', '<enwiki>']})    # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b542ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize(examples: LazyBatch) -> BatchEncoding:\n",
    "    \"\"\"\n",
    "    Tokenizes the input data by concatenating the item, description, and enwiki_extract fields and separating them with the special tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    results: list[str] = []\n",
    "    for item, description, enwiki in zip(examples['item'], examples['description'], examples['enwiki_extract']):    # type: ignore\n",
    "        # Concatenate the item, description, and enwiki_extract fields\n",
    "        results.append(f\"<item> {item} <description> {description} <enwiki> {enwiki}\")\n",
    "\n",
    "    return tokenizer(results, truncation = True)\n",
    "\n",
    "train_data: Dataset = Dataset.from_pandas(train_df).map(tokenize, batched = True)\n",
    "val_data: Dataset = Dataset.from_pandas(val_df).map(tokenize, batched = True)\n",
    "test_data: Dataset = Dataset.from_pandas(test_df).map(tokenize, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176ff1c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80b268",
   "metadata": {},
   "source": [
    "### Tuning and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "pretrained_model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                                                       num_labels = len(label_encoder.classes_),\n",
    "                                                                                       ignore_mismatched_sizes = True\n",
    "                                                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the model embeddings\n",
    "pretrained_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633993f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize the hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate: float = trial.suggest_float('learning_rate', 1e-6, 1e-4, log = True)\n",
    "    weight_decay: float = trial.suggest_float('weight_decay', 1e-6, 0.1, log = True)\n",
    "    warmup_steps: int = trial.suggest_int('warmup_steps', 0, 1000, step = 100)\n",
    "\n",
    "    # Initialize wandb\n",
    "    model_utils.configure_wandb(project = 'Cultural classification on text',\n",
    "                                name = f'trial_{trial.number}',\n",
    "                                library = 'transformers'\n",
    "                                )\n",
    "\n",
    "    # Trainer\n",
    "    trainargs: TrainingArguments = TrainingArguments(num_train_epochs = 1000,    # Unlimited epochs\n",
    "                                                     learning_rate = learning_rate,\n",
    "                                                     weight_decay = weight_decay,\n",
    "                                                     warmup_steps = warmup_steps,\n",
    "                                                     auto_find_batch_size = True,\n",
    "                                                     fp16 = True,\n",
    "                                                     metric_for_best_model = 'f1',\n",
    "                                                     load_best_model_at_end = True,\n",
    "                                                     eval_strategy = 'steps',\n",
    "                                                     eval_steps = 100,\n",
    "                                                     save_strategy = 'best',\n",
    "                                                     save_total_limit = 1,\n",
    "                                                     output_dir = str(paths.TRANSFORMER_MODEL_DIR),\n",
    "                                                     report_to = 'wandb'\n",
    "                                                     )\n",
    "\n",
    "    trainer: Trainer = Trainer(callbacks = [EarlyStoppingCallback(early_stopping_patience = 10)],\n",
    "                               model = pretrained_model,\n",
    "                               args = trainargs,\n",
    "                               train_dataset = train_data,\n",
    "                               eval_dataset = val_data,\n",
    "                               processing_class = tokenizer,\n",
    "                               compute_metrics = models_creation.transformer_metrics\n",
    "                               )\n",
    "\n",
    "    # Train the model   \n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1: float\n",
    "    try:\n",
    "        f1 = wandb.summary['eval/f1']['max']    # type: ignore\n",
    "    except:\n",
    "        raise optuna.TrialPruned(\"No f1 score found in wandb logger. Probably something went wrong during training.\")\n",
    "    finally:\n",
    "        # Close the wandb run\n",
    "        wandb.finish()\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a90223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna study\n",
    "study: optuna.Study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(objective, n_trials = 10, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter importances\n",
    "param_importances_fig: Figure = plot_param_importances(study)\n",
    "param_fig: Figure = plot_param_importances(study)\n",
    "param_fig.update_layout(autosize = False,\n",
    "                        width = 1200,\n",
    "                        height = 400\n",
    "                        )\n",
    "param_fig.show()\n",
    "\n",
    "# Plot contour\n",
    "contour_fig: Figure = plot_contour(study)\n",
    "contour_fig.update_layout(autosize = False,\n",
    "                          width = 1200,\n",
    "                          height = 1200\n",
    "                          )\n",
    "contour_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "best_params: dict[str, int|float] = study.best_trial.params\n",
    "best_lr: float = best_params['learning_rate']\n",
    "best_weight_decay: float = best_params['weight_decay']\n",
    "best_warmup_steps: float = best_params['warmup_steps']\n",
    "\n",
    "print(f\"\"\"Best hyperparameters:\n",
    "      \\tlearning rate: {best_lr:.3e}\n",
    "      \\tweight decay: {best_weight_decay:.3e}\n",
    "      \\twarmup steps: {best_warmup_steps}\"\"\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf1543",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(get_last_checkpoint(paths.TRANSFORMER_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd76ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "results_trainer: Trainer = Trainer(model = model,\n",
    "                                   args = TrainingArguments(auto_find_batch_size = True,\n",
    "                                                            output_dir = str(paths.TRANSFORMER_MODEL_DIR),\n",
    "                                                            report_to = 'none'\n",
    "                                                            ),\n",
    "                                   eval_dataset = val_data,\n",
    "                                   processing_class = tokenizer,\n",
    "                                   compute_metrics = models_creation.transformer_metrics\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f067246",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4946d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_results: dict[str, float] = results_trainer.evaluate()\n",
    "print(f\"Loss: {val_results['eval_loss']:.3f}\")\n",
    "print(f\"Accuracy: {val_results['eval_accuracy']:.3f}\")\n",
    "print(f\"F1 score: {val_results['eval_f1']:.3f}\")\n",
    "print(f\"Precision: {val_results['eval_precision']:.3f}\")\n",
    "print(f\"Recall: {val_results['eval_recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "val_logits: NDArray[np.float32] = np.array(results_trainer.predict(val_data).predictions)    # type: ignore\n",
    "val_predictions_encoded: NDArray[np.intp] = np.argmax(val_logits, axis = 1)\n",
    "model_utils.plot_confusion_matrix(val_y_encoded, val_predictions_encoded, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf80493",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions on the test set\n",
    "test_logits: NDArray[np.float32] = np.array(results_trainer.predict(test_data).predictions)    # type: ignore\n",
    "test_predictions_encoded: NDArray[np.intp] = np.argmax(test_logits, axis = 1)\n",
    "test_predictions: NDArray[str] = label_encoder.inverse_transform(test_predictions_encoded)  # type: ignore\n",
    "\n",
    "# Save the predictions\n",
    "test_predictions_df: pd.DataFrame = pd.DataFrame({'item': full_test_set['item'],\n",
    "                                                  'name': full_test_set['name'],\n",
    "                                                  'label': test_predictions\n",
    "                                                  }, index = full_test_set.index\n",
    "                                                  )\n",
    "test_predictions_df.to_csv(paths.TRANSFORMER_PREDICITONS, index_label = 'id')\n",
    "print(f\"Saved the predicitons on test set to {paths.TRANSFORMER_PREDICITONS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
