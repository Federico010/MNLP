{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50e4b07",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2400b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets.formatting.formatting import LazyBatch\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.training_args import TrainingArguments\n",
    "import wandb\n",
    "\n",
    "from modules import dataset, models_creation, paths\n",
    "from modules.utils import model as model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac4fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for asyncio to be run in notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name: str = 'sentence-transformers/paraphrase-TinyBERT-L6-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f2432",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0a45b",
   "metadata": {},
   "source": [
    "### Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "full_train_set: pd.DataFrame = dataset.extract_dataset('train')\n",
    "full_val_set: pd.DataFrame = dataset.extract_dataset('validation')\n",
    "full_test_set: pd.DataFrame = dataset.extract_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b48d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split x and y\n",
    "train_x: pd.DataFrame = full_train_set.filter(regex = '_extract$')\n",
    "train_y: pd.Series = full_train_set['label']\n",
    "val_x: pd.DataFrame = full_val_set.filter(regex = '_extract$')\n",
    "val_y: pd.Series = full_val_set['label']\n",
    "test_x: pd.DataFrame = full_test_set.filter(regex = '_extract$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a63ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "train_x = train_x.rename(columns = lambda x: x.replace('_extract', ''))\n",
    "val_x = val_x.rename(columns = lambda x: x.replace('_extract', ''))\n",
    "test_x = test_x.rename(columns = lambda x: x.replace('_extract', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93b8af",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc086b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder: LabelEncoder = LabelEncoder()\n",
    "train_y_encoded: pd.Series = pd.Series(label_encoder.fit_transform(train_y),    # type: ignore\n",
    "                                       name = train_y.name,\n",
    "                                       index = train_y.index\n",
    "                                       )\n",
    "val_y_encoded: pd.Series = pd.Series(label_encoder.transform(val_y),    # type: ignore\n",
    "                                     name = val_y.name,\n",
    "                                     index = val_y.index\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "train_df: pd.DataFrame = pd.concat([train_x, train_y_encoded], axis = 1)\n",
    "val_df: pd.DataFrame = pd.concat([val_x, val_y_encoded], axis = 1)\n",
    "test_df: pd.DataFrame = test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71798dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tokenizer\n",
    "tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add the additional special tokens\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [f'<{col}>' for col in train_x.columns]})    # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d267daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator\n",
    "data_collator: DataCollatorWithPadding = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b542ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "def tokenize(examples: LazyBatch) -> BatchEncoding:\n",
    "    \"\"\"\n",
    "    Tokenizes the input data by concatenating the values of each row and adding the column names tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    rows: list[str] = []\n",
    "    \n",
    "    # Iterate over the rows of the input data\n",
    "    for values in zip(*(examples[col] for col in train_x.columns)):\n",
    "        processed_extracts: list[str] = []\n",
    "        # Iterate over the values of each row\n",
    "        for col, value in zip(train_x.columns, values):\n",
    "            extract: str = f'<{col}> {value}' if value else f'<{col}>'\n",
    "            processed_extracts.append(extract)\n",
    "        rows.append(' '.join(processed_extracts))\n",
    "\n",
    "    # Tokenize the concatenated strings\n",
    "    return tokenizer(rows, truncation = True)\n",
    "\n",
    "train_data: Dataset = Dataset.from_pandas(train_df).map(tokenize, batched = True)\n",
    "val_data: Dataset = Dataset.from_pandas(val_df).map(tokenize, batched = True)\n",
    "test_data: Dataset = Dataset.from_pandas(test_df).map(tokenize, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0176ff1c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80b268",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "pretrained_model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                                            num_labels = len(label_encoder.classes_),\n",
    "                                                                            ignore_mismatched_sizes = True\n",
    "                                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the tokenizer embeddings\n",
    "pretrained_model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of = 8)    # ideal for fp16 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bbeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project = 'Cultural classification on text', dir = paths.DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86729cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "trainargs: TrainingArguments = TrainingArguments(num_train_epochs = 1000,    # Unlimited epochs\n",
    "                                                 auto_find_batch_size = True,\n",
    "                                                 fp16 = True,\n",
    "                                                 metric_for_best_model = 'f1',\n",
    "                                                 load_best_model_at_end = True,\n",
    "                                                 eval_strategy = 'steps',\n",
    "                                                 eval_steps = 100,\n",
    "                                                 save_strategy = 'best',\n",
    "                                                 save_total_limit = 1,\n",
    "                                                 output_dir = str(paths.TRANSFORMER_MODEL_DIR),\n",
    "                                                 report_to = 'wandb'\n",
    "                                                 )\n",
    "\n",
    "trainer: Trainer = Trainer(callbacks = [EarlyStoppingCallback(early_stopping_patience = 10)],\n",
    "                           model = pretrained_model,\n",
    "                           args = trainargs,\n",
    "                           train_dataset = train_data,\n",
    "                           eval_dataset = val_data,\n",
    "                           processing_class = tokenizer,\n",
    "                           data_collator = data_collator,\n",
    "                           compute_metrics = models_creation.transformer_metrics\n",
    "                           )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5cb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf1543",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eea07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(get_last_checkpoint(paths.TRANSFORMER_MODEL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd76ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "results_trainer: Trainer = Trainer(model = model,\n",
    "                                   args = TrainingArguments(auto_find_batch_size = True,\n",
    "                                                            output_dir = str(paths.TRANSFORMER_MODEL_DIR),\n",
    "                                                            report_to = 'none'\n",
    "                                                            ),\n",
    "                                   eval_dataset = val_data,\n",
    "                                   processing_class = tokenizer,\n",
    "                                   data_collator = data_collator,\n",
    "                                   compute_metrics = models_creation.transformer_metrics\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f067246",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4946d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_results: dict[str, float] = results_trainer.evaluate()\n",
    "print(f\"Loss: {val_results['eval_loss']:.3f}\")\n",
    "print(f\"Accuracy: {val_results['eval_accuracy']:.3f}\")\n",
    "print(f\"F1 score: {val_results['eval_f1']:.3f}\")\n",
    "print(f\"Precision: {val_results['eval_precision']:.3f}\")\n",
    "print(f\"Recall: {val_results['eval_recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "val_logits: NDArray[np.float32] = np.array(results_trainer.predict(val_data).predictions)    # type: ignore\n",
    "val_predictions_encoded: NDArray[np.intp] = np.argmax(val_logits, axis = 1)\n",
    "model_utils.plot_confusion_matrix(val_y_encoded, val_predictions_encoded, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf80493",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions on the test set\n",
    "test_logits: NDArray[np.float32] = np.array(results_trainer.predict(test_data).predictions)    # type: ignore\n",
    "test_predictions_encoded: NDArray[np.intp] = np.argmax(test_logits, axis = 1)\n",
    "test_predictions: NDArray[str] = label_encoder.inverse_transform(test_predictions_encoded)  # type: ignore\n",
    "\n",
    "# Save the predictions\n",
    "test_predictions_df: pd.DataFrame = pd.DataFrame({'item': full_test_set['item'],\n",
    "                                                  'name': full_test_set['name'],\n",
    "                                                  'label': test_predictions\n",
    "                                                  }, index = full_test_set.index\n",
    "                                                  )\n",
    "test_predictions_df.to_csv(paths.TRANSFORMER_PREDICITONS, index_label = 'id')\n",
    "print(f\"Saved the predicitons on test set to {paths.TRANSFORMER_PREDICITONS}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
