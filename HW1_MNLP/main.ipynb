{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import nest_asyncio\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.visualization import plot_param_importances, plot_contour\n",
    "import pandas as pd\n",
    "from plotly.graph_objects import Figure\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from sklearn.preprocessing import LabelEncoder, MaxAbsScaler\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import wandb\n",
    "\n",
    "from modules import dataset, graph, model, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330ae1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows for asyncio to be run in notebooks\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570449bf",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af026745",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "# Preprocessors\n",
    "scaler: MaxAbsScaler = MaxAbsScaler()\n",
    "label_encoder: LabelEncoder = LabelEncoder()\n",
    "\n",
    "# Prepare the training set\n",
    "train_set: pd.DataFrame = dataset.get_page_len_dataset('train')\n",
    "train_x: pd.DataFrame = train_set.drop(columns=['label'])\n",
    "train_x = pd.DataFrame(scaler.fit_transform(train_x), columns=train_x.columns)\n",
    "train_y: torch.Tensor = torch.tensor(label_encoder.fit_transform(train_set['label']))\n",
    "\n",
    "# Prepare the validation set\n",
    "valid_set: pd.DataFrame = dataset.get_page_len_dataset('valid')\n",
    "valid_x: pd.DataFrame = valid_set.drop(columns=['label']).reindex(columns=train_x.columns)\n",
    "valid_x = pd.DataFrame(scaler.transform(valid_x), columns=valid_x.columns)\n",
    "valid_y: torch.Tensor = torch.tensor(label_encoder.transform(valid_set['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graphs\n",
    "\n",
    "similarity_graph: graph.SimilarityGraph = graph.SimilarityGraph(train_x, k = 5, show = True)\n",
    "\n",
    "training_graphs: list[nx.DiGraph] = similarity_graph.get_graphs(train_x.fillna(0))\n",
    "validation_graphs: list[nx.DiGraph] = similarity_graph.get_graphs(valid_x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the graphs compatible with PyTorch Geometric\n",
    "\n",
    "train_data: list[Data] = []\n",
    "for digraph, label in zip(training_graphs, train_y):\n",
    "    data: Data = from_networkx(digraph)\n",
    "    data.y = label\n",
    "    train_data.append(data)\n",
    "\n",
    "valid_data: list[Data] = []\n",
    "for digraph, label in zip(validation_graphs, valid_y):\n",
    "    data: Data = from_networkx(digraph)\n",
    "    data.y = label\n",
    "    valid_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652f4f1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0ef91",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize the hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    first_layer_channels: int = trial.suggest_int('first_layer_channels', 32, 256, step = 32)\n",
    "    lr: float = trial.suggest_float('lr', 1e-5, 1e-1, log = True)\n",
    "    dropout: float = trial.suggest_float('dropout', 0., 0.5)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader: DataLoader = DataLoader(train_data, batch_size = 256, shuffle = True)\n",
    "    valid_loader: DataLoader = DataLoader(valid_data, batch_size = 256)\n",
    "\n",
    "    # Model\n",
    "    gcn: model.GCN = model.GCN(first_layer_channels, lr = lr, dropout = dropout)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping: EarlyStopping = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "    pruning_callback: PyTorchLightningPruningCallback = PyTorchLightningPruningCallback(trial, monitor = 'val_loss')\n",
    "\n",
    "    # Trainer\n",
    "    trainer: pl.Trainer = pl.Trainer(max_epochs = -1,\n",
    "                                     callbacks = [early_stopping, pruning_callback],\n",
    "                                     logger = False,\n",
    "                                     enable_progress_bar = False,\n",
    "                                     enable_model_summary = False,\n",
    "                                     enable_checkpointing = False,\n",
    "                                     precision = '16-mixed'\n",
    "                                     )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(gcn, train_loader, valid_loader)\n",
    "\n",
    "    pruning_callback.check_pruned()\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1: float = trainer.validate(gcn, valid_loader, verbose = False)[0]['val_f1']\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna study\n",
    "study: optuna.Study = optuna.create_study(direction = 'maximize', pruner = optuna.pruners.MedianPruner(n_warmup_steps = 10))\n",
    "study.optimize(objective, n_trials = 10, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb58746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter importances\n",
    "param_importances_fig: Figure = plot_param_importances(study)\n",
    "param_fig: Figure = plot_param_importances(study)\n",
    "param_fig.update_layout(autosize = False,\n",
    "                        width = 1200,\n",
    "                        height = 400\n",
    "                        )\n",
    "param_fig.show()\n",
    "\n",
    "# Plot contour\n",
    "contour_fig: Figure = plot_contour(study)\n",
    "contour_fig.update_layout(autosize = False,\n",
    "                          width = 1200,\n",
    "                          height = 1200\n",
    "                          )\n",
    "contour_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2392f",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e263a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the best hyperparameters and print them\n",
    "best_params: dict[str, int|float] = study.best_trial.params\n",
    "\n",
    "# Retrain the model with the best hyperparameters\n",
    "best_channels: int = int(best_params['first_layer_channels'])\n",
    "best_lr: float = best_params['lr']\n",
    "best_dropout: float = best_params['dropout']\n",
    "\n",
    "print(f\"Best hyperparameters:\\n\\tfirst layer channels channels: {best_channels}\\n\\tlearning rate: {best_lr:.3e}\\n\\tdropout: {best_dropout:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43873c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the best hyperparameters\n",
    "\n",
    "# Data loaders\n",
    "train_loader: DataLoader = DataLoader(train_data,\n",
    "                                      batch_size = 256,\n",
    "                                      shuffle = True\n",
    "                                      )\n",
    "valid_loader: DataLoader = DataLoader(valid_data,\n",
    "                                      batch_size = 256\n",
    "                                      )\n",
    "\n",
    "# Model\n",
    "gcn: model.GCN = model.GCN(best_channels, lr = best_lr, dropout = best_dropout)\n",
    "\n",
    "# Wandb logger\n",
    "wandb_logger: WandbLogger = WandbLogger(project = 'MNLP_HW_1', name = 'best_model', save_dir = paths.DATA_DIR)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping: EarlyStopping = EarlyStopping(monitor='val_loss', patience = 10)\n",
    "checkpoint: ModelCheckpoint = ModelCheckpoint(monitor='val_loss')\n",
    "\n",
    "# Trainer\n",
    "trainer: pl.Trainer = pl.Trainer(max_epochs = -1,\n",
    "                                 callbacks = [early_stopping, checkpoint],\n",
    "                                 logger = wandb_logger,\n",
    "                                 log_every_n_steps = len(train_loader)\n",
    "                                 )\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(gcn, train_loader, valid_loader)\n",
    "\n",
    "# Close wandb and remove the logger from the trainer\n",
    "trainer.logger = None\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f03d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "gcn: model.GCN = model.GCN.load_from_checkpoint(checkpoint.best_model_path)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "trainer.validate(gcn, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
