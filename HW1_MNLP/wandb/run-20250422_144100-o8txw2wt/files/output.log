XLMRobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
[W 2025-04-22 14:48:39,274] Trial 6 failed with parameters: {'learning_rate': 3.625233268751596e-06, 'per_device_train_batch_size': 8, 'weight_decay': 0.009262294718761336, 'warmup_steps': 17, 'num_train_epochs': 2} because of the following error: ValueError('all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 4 dimension(s)').
Traceback (most recent call last):
  File "C:\Users\fedur\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\optuna\study\_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "C:\Users\fedur\AppData\Local\Temp\ipykernel_25688\1242884043.py", line 57, in objective
    logits = np.concatenate(raw_preds, axis=0)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 4 dimension(s)
[W 2025-04-22 14:48:39,335] Trial 6 failed with value None.
